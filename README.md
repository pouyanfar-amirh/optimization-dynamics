# Optimization and Dynamics of Learning

This project explores how the choice of optimization method influences the convergence and generalization of deep models.  

Datasets: MNIST, CIFAR-10  
Optimizers: SGD, Momentum, RMSProp, Adam  
Learning rate schedules, sharp vs. flat minima  
